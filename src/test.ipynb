{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnw\\anaconda3\\envs\\deeplearn\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\johnw\\anaconda3\\envs\\deeplearn\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 202/202 [04:11<00:00,  1.24s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 51/51 [01:02<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing pairwise diatances ...\n",
      "WARNING: 21 negative diff_squares found and set to zero, min_diff_square= tensor(-9.0949e-13, dtype=torch.float64)\n",
      "Initialization is DONE !\n",
      "[52.22301369 50.24004482 50.17362354 ...  9.92718466  9.40926813\n",
      "  8.67602802]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from rarity_score import *\n",
    "from torchvision import datasets, transforms, models\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "seed = 2302\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class WiderFaceDataset(Dataset):\n",
    "    def __init__(self, image_dir, annotation_file, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.annotations = self.load_annotations(annotation_file)\n",
    "        self.images = [annotation['image'] for annotation in self.annotations]\n",
    "\n",
    "    def load_annotations(self, annotation_file):\n",
    "        annotations = []\n",
    "        with open(annotation_file, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            i = 0\n",
    "            while i < len(lines):\n",
    "                # read the image filename\n",
    "                image_path = lines[i].strip()\n",
    "                i += 1\n",
    "\n",
    "                # the number of faces\n",
    "                num_faces = int(lines[i].strip())\n",
    "                i += 1  # Move to the first bounding box line or skip if num_faces is 0\n",
    "\n",
    "                # if no faces, skip this image\n",
    "                if num_faces == 0:\n",
    "                    i += 1\n",
    "                    continue\n",
    "\n",
    "                boxes = []\n",
    "                for _ in range(num_faces):\n",
    "                    box_info = lines[i].strip().split()\n",
    "                    # Extract the first four values (x, y, width, height)\n",
    "                    x, y, w, h = [int(box_info[k]) for k in range(4)]\n",
    "                    boxes.append([x, y, x + w, y + h])\n",
    "                    i += 1\n",
    "\n",
    "                annotations.append({'image': image_path, 'boxes': boxes})\n",
    "\n",
    "        return annotations\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.image_dir, self.annotations[index]['image'])\n",
    "        img = Image.open(img_path)\n",
    "\n",
    "        original_size = img.size\n",
    "\n",
    "        # corresponding bounding boxes\n",
    "        bboxes = self.annotations[index]['boxes']\n",
    "\n",
    "        # apply image transformations()\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        # scale factors\n",
    "        scale_x = 299 / original_size[0]\n",
    "        scale_y = 299 / original_size[1]\n",
    "\n",
    "        # scale the bounding boxes\n",
    "        scaled_bboxes = []\n",
    "        for box in bboxes:\n",
    "            x1, y1, x2, y2 = box\n",
    "            scaled_bboxes.append([x1 * scale_x, y1 * scale_y, x2 * scale_x, y2 * scale_y])\n",
    "\n",
    "        # convert scaled bounding boxes(annotations) to a tensor\n",
    "        scaled_bboxes = torch.tensor(scaled_bboxes, dtype=torch.float32)\n",
    "\n",
    "        return img, scaled_bboxes\n",
    "\n",
    "transform_inception = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# load datasets\n",
    "train_wider_dataset = WiderFaceDataset(image_dir='./widerface_data/widerface/WIDER_train/images/', \n",
    "                                 annotation_file='./widerface_data/widerface/wider_face_split/wider_face_train_bbx_gt.txt', \n",
    "                                 transform=transform_inception)\n",
    "test_wider_dataset = WiderFaceDataset(image_dir='./widerface_data/widerface/WIDER_val/images/', \n",
    "                                 annotation_file='./widerface_data/widerface/wider_face_split/wider_face_val_bbx_gt.txt', \n",
    "                                 transform=transform_inception)\n",
    "\n",
    "# train_wider_dataset = datasets.WIDERFace(\"./widerface_data\", split =\"train\", download=True, transform = transform_inception)\n",
    "# test_wider_dataset = datasets.WIDERFace(\"./widerface_data\", split =\"test\", download=True, transform = transform_inception)\n",
    "\n",
    "# load feature extractor\n",
    "inception = models.inception_v3(pretrained=True).to(device)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = [item[0] for item in batch]\n",
    "    annotations = [item[1] for item in batch]  # handle varying-size annotations\n",
    "\n",
    "    # stack the images into a single tensor\n",
    "    images = torch.stack(images, dim=0)\n",
    "\n",
    "    return images, annotations\n",
    "\n",
    "\n",
    "train_wider_loader = DataLoader(train_wider_dataset, batch_size=64, collate_fn=collate_fn)\n",
    "test_wider_loader = DataLoader(test_wider_dataset, batch_size=64, collate_fn=collate_fn)\n",
    "\n",
    "def extract_features(model, data_loader):\n",
    "    model.eval()\n",
    "    features = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader):\n",
    "            images = batch[0].to(device)\n",
    "            output = model(images)\n",
    "            features.append(output.cpu())\n",
    "    return torch.cat(features, dim=0)\n",
    "\n",
    "real_wider_features = extract_features(inception, train_wider_loader)\n",
    "fake_wider_features = extract_features(inception, test_wider_loader)\n",
    "\n",
    "real_wider_features = real_wider_features.numpy()\n",
    "fake_wider_features = fake_wider_features.numpy()\n",
    "\n",
    "manifold = MANIFOLD(real_features=real_wider_features, fake_features=fake_wider_features)\n",
    "score, score_index = manifold.rarity(k=3)\n",
    "print(score[score_index])\n",
    "np.savetxt('./result/inception_widerface.txt', score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
